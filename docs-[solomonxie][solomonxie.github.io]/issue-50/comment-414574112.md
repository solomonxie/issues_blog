# `What is Random Variable`

> Instead of analyzing a **measured** distribution with explicit data, we're to **abstract** those analysis methods with **uncertain** data.
It's like **abstracting `arithmetic` to `algebra`**.

`Random Variables` are just like the **unknowns** in algebra. Except it's slightly different in Statistics.

Remember that: Studying `Random Variables` is just like studying `Algebra` over Arithmetic.


## `Expected Value` of a Random Variable
> It's also known as the `Expectation`, `Mathematical Expectation`, `EV`, `Average`, `Mean Value`, `Mean`, or `First Moment`.

[Refer to wiki: Expected value](https://www.wikiwand.com/en/Expected_value)

"In probability theory, the expected value of a random variable, intuitively, is the long-run average value of repetitions of the experiment it represents."


## `Algebra of Random Variables`
> Some basic operations for random variables.

[Refer to wiki: Algebra of random variables](https://www.wikiwand.com/en/Algebra_of_random_variables)
[Refer to article on Khan academy: Combining random variables](https://www.khanacademy.org/math/ap-statistics/random-variables-ap/modal/a/combining-random-variables-article)

![image](https://user-images.githubusercontent.com/14041622/44387928-6fbb0500-a559-11e8-9f82-fd8ac5f11411.png)

Important facts about combining variances:
- The variables must be independent to each other.
- We can find the `standard deviation` by taking square root âˆš of the combined variances.
- The variance **increases** even when we subtract random variables.

