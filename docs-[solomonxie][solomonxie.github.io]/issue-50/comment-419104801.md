# `Null Hypothesis Significance Testing`

`Hypothesis Testing` is that we make a assumption, or a hypothesis about something, and we then make a test and do statistic on it as evidence to **against** the hypothesis. 

> We can **NEVER** prove the null hypothesis, because "INNOCENT UNTIL PROVEN GUILTY".

[Refer to youtube: What is a Hypothesis Test and a P-Value?](https://www.youtube.com/watch?v=vwWEa8wU_6U&t=381s)

`Null hypothesis significance testing` is 


## `Null Hypothesis` & `Alternative Hypothesis`

[Refer to youtube: Hypothesis Testing 2: null and alternative hypothesis (one sample t test)](https://www.youtube.com/watch?v=L1GV6nLnbyE)

Notations:
- H₀: Null Hypothesis (reads `H-knot`)
_Null hypothesis_ is the assumption we claimed as our opinion.
- Ha: Alternative Hypothesis (reads `H-alternative`)
_Alternative hypothesis_ is the **opposition** against the _null hypothesis_.

etc., if the _null hypothesis_ is "Jason's IQ is 130", then the _alternative hypothesis_ is "his IQ is below 130".


## `Significance Level ⍺` (Threshold)

For making a "judge" on wether the hypothesis stands or fails, we need a standard or threshold to judge it, which we called the `Significance Level`, or the `Cutoff`, denoted `⍺` (alpha).

There are a few common sets on the _significance level_:
- \<1%: Very strong evidence against our claim.
- \<5%: Strong evidence against our claim.
- \<10%: Weak evidence against our claim.
- \>10%: Little or no evidence against our claim.

![image](https://user-images.githubusercontent.com/14041622/45202814-be340780-b2ac-11e8-9fa0-3b3dc088877b.png)

### `Critical Values` & `Rejection Regions`
[Refer to youtube: Hypothesis Testing 4: critical values and rejection regions (one sample t test)](https://www.youtube.com/watch?v=BdeuCflLPQI)

![image](https://user-images.githubusercontent.com/14041622/45207719-5802b100-b2bb-11e8-85cd-ca0e59bb65a9.png)



### Example
![image](https://user-images.githubusercontent.com/14041622/45251250-7f1db900-b375-11e8-961c-79112367581a.png)
Solve:
- According to the table, out of 100010001000 simulated samples:
    - 5 had 80%, percent satisfied customers
    - None had a lower measured percentage of satisfied customers
- In total, these sum up to 5 simulations out of 1000. Therefore, the simulations imply that the probability of having a sample with 80%, percent satisfied customers or less is:
![image](https://user-images.githubusercontent.com/14041622/45251265-c7d57200-b375-11e8-8a06-a2b8f79e7ec5.png)
- The probability we got is lower than 1%, percent. Therefore, we should reject the hypothesis.


### Example
![image](https://user-images.githubusercontent.com/14041622/45251367-762de700-b377-11e8-9444-790d3d9b6b8b.png)
Solve:
- Assume the 40% probability is **TRUE**,
- so the probability of getting 3 wins in a row is: `40%^3 = 6.4%`
- Therefore we SHOULDN'T reject it, because it's higher than 5%.


## `p-value`
> p-value stands for _Probability value_.

[Refer to youtube: Hypothesis Testing 5: p values (one sample t test)](https://www.youtube.com/watch?v=WojcyhC7EVc)

_p-value_ is the probability of the null hypothesis occurring.

> It tells **how rare** your data is if the _null hypothesis_ was true.That being said, _p-value_ is the measure of the strength of the evidence **against** the _null hypothesis_.

The smaller the _p-value_, the greater the evidence against the _null hypothesis_.

> p-value is a **`Conditional Probability`**.
[`▶︎ Back to previous note: Conditional Probability`](https://github.com/solomonxie/solomonxie.github.io/issues/50#issuecomment-412445737)

### `How to understand p-value`
> It's saying that, based on your theory, the reality is almost impossible with 0.49% of chance occurs. But reality is reality, in another word, it's more about saying the theory itself is so wrong.

Why do we count the _proportion of the tail_ instead of the proportion in between of the theory & reality?
_proportion of the tail_ represents the _reality value adding up with all the "more rare" values, that kinda makes sense.

![image](https://user-images.githubusercontent.com/14041622/45208130-71582d00-b2bc-11e8-97b8-38546dff6fb6.png)


### `How to get p-value`

![image](https://user-images.githubusercontent.com/14041622/45206227-17089d80-b2b7-11e8-98ec-3d413f8d7443.png)

- First we assume the original assumption (__null hypothesis__) is **true**
- and we build a _normal distribution_ on the hypothesis
- then we take a sample and calculate statistic on it
- followed by drawing the _sample statistic_ on the "_hypothesis distribution_"
- so we calculate the **proportion of the tail** from the point we drew for the _sample statistic_.
- finally we get the **probability**, as the `p-value`.

![image](https://user-images.githubusercontent.com/14041622/45207840-b62f9400-b2bb-11e8-9ea6-87b7dd1c69de.png)



## `How  to do Hypothesis Testing`

[Refer to article on Khan academy: Using P-values to make conclusions](https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample/modal/a/p-value-conclusions)

![image](https://user-images.githubusercontent.com/14041622/45216112-0a457300-b2d2-11e8-8d7d-b741ca3b2241.png)

![image](https://user-images.githubusercontent.com/14041622/45165275-14f6fe00-b227-11e8-97e1-fe7854f01b08.png)



![image](https://user-images.githubusercontent.com/14041622/45207510-b67b5f80-b2ba-11e8-9e63-f9d0a3a65497.png)

