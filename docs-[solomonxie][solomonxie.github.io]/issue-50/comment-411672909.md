# `R-Squared`
> `R-squared` means **squared residuals**, is also called `SE`, which is **Squared Errors**.

[Refer to Khan academy: R-squared or coefficient of determination](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/modal/v/r-squared-or-coefficient-of-determination)

![image](https://user-images.githubusercontent.com/14041622/43886435-449b4764-9bee-11e8-9996-b05c33876e04.png)

`Coefficient of Determinator` (`r²` or `R-squared`):
![image](https://user-images.githubusercontent.com/14041622/43886827-8d98166c-9bef-11e8-8443-45a2830a67e9.png)
(SE_line is `Squared Error from line`, )

- If `SE` (Squared-Error) from the line is **small**   ->   r² close to 1   ->  The line is a **good fit**.
- If `SE` (Squared-Error) from the line is **large**   ->   r² close to 0   ->    The line is **not** a good fit

## Understanding R-squared
[Refer to youtube: 3.2: Linear Regression with Ordinary Least Squares Part 1 - Intelligence and Learning](https://www.youtube.com/watch?v=szXbuO3bVRk)


### Why do we square _Residuals_?
It's just a way to keep those `residuals` (difference from the regression line) **positive**.
And actually the `residuals` or `squared residuals` DOESN'T really matter to us, 
because we're to **MINIMIZE** them anyway. Take the minimum residual or minimum residual squared doesn't matter.


### Why do we square _Correlation Coefficient_?


### Why do we add them together
By adding them we will get the **TOTAL ERRORS**, which is the one we're going to **minimize**.



