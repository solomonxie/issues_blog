# `Least-square Regression`
> _Least-square Regression_ is one way of calculating _Linear Regression_. 
Most regressions' calculations are done by **computer**, but we want to do that by hand to have better understanding.

What is Linear Regression?
Trying to fit a line as closely as possible, and as many of points as possible, is called "Linear Regression".

[Refer to Khan academy: Introduction to residuals and least-squares regression](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/modal/v/regression-residual-intro)

![image](https://user-images.githubusercontent.com/14041622/45735579-ef042d00-bc1a-11e8-97dd-eb5bf7a27f70.png)



## `Residuals`
> Residuals are **errors**. More specifically, they are the differences between the actual value of the response variable and the value predicted by the least squares regression line.

![image](https://user-images.githubusercontent.com/14041622/45736304-24aa1580-bc1d-11e8-8305-db1d5d1a3a4b.png)

At a certain X-position, the value of _residual_ is the **VERTICAL DISTANCE** from the actual value to the Regression Line.

- When the `residual` is **positive**, the **actual point** is **ABOVE** the `regression line`,
- When the `residual` is **negative**, the **actual point** is **BELOW** the `regression line`.

![image](https://user-images.githubusercontent.com/14041622/45735633-1b1fae00-bc1b-11e8-9b41-477c1a38572e.png)

> The way that we calculate the `Regression Line` with `Least Square` method, is to **MINIMIZE** the **square of residuals**.


### Example
![image](https://user-images.githubusercontent.com/14041622/45736405-620ea300-bc1d-11e8-8797-14c26fdf6053.png)
Solve:
- This dish's actual taste rating was 4 points higher than predicted **based on its appearance**


## `Least Square Regression`
> `Least Square` is a the most common method to calculate the Linear Regression Line.

[Refer to Khan academy: Calculating the equation of a regression line](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/modal/v/calculating-the-equation-of-a-regression-line)

Formula of Regression line:
![image](https://user-images.githubusercontent.com/14041622/43884348-77a305d0-9be8-11e8-9fc1-5bd881686fb4.png)

1. As we said the `Correlation Coefficient r` is kind like the **`Unit Slope`** which is between `-1 to 1`, so we have to apply the `unit slope` in real case by multiply `r` with the **ratio** of Standard Deviation of `y` & `x`, which is `Sy/Sx`.
2. A "must go through point" is the **MEAN** of the dataset, which is: `(Ẋ, Ẏ)`. At the mean, the `residual = actual`

With two informations above, we can easily calculate out the estimated Regression Line.


### Why are we squaring residuals?
It's just a way to keep those `residuals` (difference from the regression line) **positive**.
And actually the `residuals` or `squared residuals` DOESN'T really matter to us, 
because we're to **MINIMIZE** them anyway. Take the minimum residual or minimum residual squared doesn't matter.



## `Slope of Regression line`

![image](https://user-images.githubusercontent.com/14041622/45735774-9a14e680-bc1b-11e8-8d9b-47aa950c391f.png)


## `Intercept of Regression line`
