# Statistical Learning (OneNote) [DRAFT]

## Basics

### Learning Goal

> WE FIRST ASSUME THERE EXISTS AN ALMIGHTY FUNCTION Y=f(x) FOR EVERY (x, y)

The goal of Statistical Learning is to  â†’  estimate  f(X), namely the á¸Ÿ(X)

Why estimate?
	- Prediction Problem
		È² = á¸Ÿ(X)
		    â†“
	          Prediction Accuracy
		    â†“
		E(Y - È²) =  Î£[ f(x) - á¸Ÿ(x) ] + Var(Îµ)
		  Reducible Error â†‘           â†‘ Irreducible Error
		
	- Inference Problem
		â—‹ Which x effect Y
		â—‹ Positive or negative effect x brings to Y (slope)
		â—‹ Linear Relationship or Non-linear


How to estimate f ?
To find out  á¸Ÿ(X) â‰ˆ Y for every (X, Y)
		â—‹ Parametric Approach
			Is to estimate the exact FORMULA of f(X)
						â†“
					Model Selection
					(Choose an exact function form)
						â†“
					Model Fitting
					(Estimating each parameters in the function)
		â—‹ Non-parametric Approach
			Directly estimate the VALUE of f(x) 
						â†“
					Choose "smothness"
						â†“
					Create a shape (etc., plane)
						â†“
					(might lead to overfitting)



### Model Selection

Model Selection
	- Interpretability
		â—‹ Interpretable (Easy to understand)
		â—‹ Predictable (More precise)
	- Labeled
		â—‹ Supervised (With right answer, labeled)  -> For prediction problem
			Â§ Quantitative -> Regression Models
			Â§ Qualitative -> Classification Models
		â—‹ Unsupervised (Without right answer)  -> For clustering analysis
			Â§ Neural Network
	- Numeric
		â—‹ Regression (Quantitative/Numerical Variables)
			Â§ Linear Regression
				â–¡ Simple Linear Regression (One variable)
				â–¡ Multiple Linear Regression (Multiple variables)
		â—‹ Classification (Qualitative/Categorical Variables)
			Â§ Classifier
				â–¡ Logistic Regression
			Â§ Bayes Classifier
			Â§ KNN Classifier
	- Accuracy (Bias-Variance Trade-off)
		â—‹ Regression settings
			Â§ MSE
			Â§ AVE
		â—‹ Classification settings
			Â§ Classifier
			Â§ Bayes Classifier
			Â§ KNN Classifier




## Statistics

### Plots

![image](https://user-images.githubusercontent.com/14041622/52002995-4d60e680-24fe-11e9-8350-985237eac321.png)



### Distribution

![image](https://user-images.githubusercontent.com/14041622/52003007-52259a80-24fe-11e9-9ec7-242e9316018c.png)

![image](https://user-images.githubusercontent.com/14041622/52003011-5487f480-24fe-11e9-961a-8dd440ba40bc.png)



### Inferential Statistics

![image](https://user-images.githubusercontent.com/14041622/52003018-6073b680-24fe-11e9-8198-88956d05d9a1.png)

![image](https://user-images.githubusercontent.com/14041622/52003024-62d61080-24fe-11e9-894b-f459cc0775e9.png)


### Probability

![image](https://user-images.githubusercontent.com/14041622/52003034-68335b00-24fe-11e9-9df8-76490b982206.png)


### Hypothesis Test [DRAFT]



### Random Variable

![image](https://user-images.githubusercontent.com/14041622/52003045-71bcc300-24fe-11e9-8066-f5eb405c08df.png)


### Bayesian Theorem [DRAFT]



## Linear Regression

### Model Accuracy

Bias-Variance Trade-Off
The prediction error for any machine learning algorithm can be broken down into three parts: 
	- Bias Error: error caused by choosing models on interpretability
		â—‹ High-Bias models: Linear Regression, Logistic Regression â€¦
		â—‹ Low-Bias models: Decision Trees, KNN, SVM
	- Variance Error: error caused by choosing models on flexibility
	- Irreducible Error (Îµ): cannot be reduced regardless of what algorithm is used. 
		â—‹ High-Variance models: Decision trees
		â—‹ Low-Variance models: Linear Regression

![image](https://user-images.githubusercontent.com/14041622/52003083-91ec8200-24fe-11e9-9a3a-d297425e7de3.png)

![image](https://user-images.githubusercontent.com/14041622/52003093-9dd84400-24fe-11e9-91cd-b787d53f0a0b.png)


### Hypothesis Test for ML

Some examples of statistical hypothesis tests and their distributions from which critical values can be calculated are as follows:
â€¢ Z-Test: Gaussian distribution (Normal Distribution).
â€¢ Student t-Test: Studentâ€™s t-distribution.
â€¢ Chi-Squared Test: Chi-Squared (ðœ²Â²) distribution.
â€¢ ANOVA: F-distribution (Fisherâ€“Snedecor distribution).

![image](https://user-images.githubusercontent.com/14041622/52003128-b6e0f500-24fe-11e9-8ce2-0f35f35ca61d.png)


### Features Selection [DRAFT]

Features selection approaches
	- Stepwise Regression
		â—‹ Forward selection
		â—‹ Backward selection
		â—‹ Bidirectional elmination
	- LASSO

â¶ Stepwise Regression
Main approaches:
	- Forward Selection
	- Backward Selection
	- Bidirectional Elimination

â· LASSO (least absolute shrinkage & selection operator)


## Classification

### Classification Basics

Encodings of Categories
	- Code with order/rank
	- Binary code: yes or no
	- One-hot Encoding


Why NOT Linear regression?
Because the probability must fall between 0 and 1,
but Linear regression is not sensible and may lead the
result below 0 or above 1.
To avoid that, we MUST model p(X) using a function gives
output between 0 and 1. 
Many functions meet this description, logistic function in 
Logistic Regression is one of them.



### Logistic Regression

> is to predict the probability of a categorical dependent variable, which is a binary variable

![image](https://user-images.githubusercontent.com/14041622/52003251-f4458280-24fe-11e9-939e-3030ee632085.png)

![image](https://user-images.githubusercontent.com/14041622/52003257-f90a3680-24fe-11e9-8767-66e0aea3d021.png)


### Linear Discriminant Analysis

LDA (linear discriminant analysis) is an alternative
to Logistic regression for the following reasons:
	- More reliable on handling more than 2 response classes
	- More stable if dataset size n is small
	- More stable if the classes are well-separated
