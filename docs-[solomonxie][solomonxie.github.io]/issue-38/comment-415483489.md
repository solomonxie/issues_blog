# `Hardwares for Deep Learning`

## Commands
- Check Storage: `df -h`
- Check CPU: `cat /proc/cpuinfo`
- Check RAM: `cat /proc/meminfo`
- Check GPU: 
```
$ sudo apt-get install nvidia-smi
$ nvidia-smi -stats
```

## `Google Colab` (Jupyter Notebook)
Link: https://colab.research.google.com/notebooks/welcome.ipynb

- GPU(12hrs): NVIDIA Tesla K80 GPU × 1
- CPU: Intel(R) Xeon(R) CPU @ 2.30GHz × 1 Core
- RAM: 13GB
- Storage: 33GB

## `Kaggle Kernel` (Jupyter Notebook)
[Refer to Kaggle: How to use Kaggle - Kenels](https://www.kaggle.com/docs/kernels)
Link: https://www.kaggle.com/kernels

- GPU: NVidia Tesla K80
- CPU: Intel(R) Xeon(R) CPU @ 2.30GHz 4 cores
- RAM: 17.2GB
- Disk: 5.2GB

## `AWS p2`
- GPU: NVIDIA Tesla K80 GPU × (1 to 16)
- CPU: Intel Xeon® E5-2686 v4 × (4 to 64)
- RAM: 61G or 488G or 732G 

> P2 instances provide up to 16 **NVIDIA K80 GPUs**, 64 vCPUs and 732 GiB of host memory, with a combined 192 GB of GPU memory, 40 thousand parallel processing cores, 70 teraflops of single precision floating point performance, and over 23 teraflops of double precision floating point performance. 
P2 instances also offer GPUDirect™ (peer-to-peer GPU communication) capabilities for up to 16 GPUs, so that multiple GPUs can work together within a single host.


![image](https://user-images.githubusercontent.com/14041622/44538961-68505300-a735-11e8-9c66-b1b26dcfa533.png)


## `DIY`
### GPU
- [`NVIDIA Tesla K80 GPU`](https://www.amazon.com/Nvidia-Tesla-GDDR5-Cores-Graphic/dp/B00Q7O7PQA): RMB 30,000, VRAM 24GB (GDDR5), 256 Bit

### CPU
- [`Intel Xeon® E5-2686 v4`](https://www.ebay.com/itm/Intel-Xeon-E5-2686-v4-SR2K8-2-30GHz-18-Core-LGA2011-3/323405561085?epid=20005107689&hash=item4b4c793cfd%3Ag%3A6cQAAOSwlQ5baCxE&LH_ItemCondition=3): RMB 12,000, 8 Cores, 1.7GHz