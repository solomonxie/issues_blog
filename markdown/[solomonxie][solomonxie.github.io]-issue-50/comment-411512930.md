# `Least Square Regression`
> Trying to fit a line as closely as possible, and as many of points as possible, is called "Linear Regression".

[Refer to Khan academy: Introduction to residuals and least-squares regression](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/modal/v/regression-residual-intro)


## `Residuals`
It is the **DISTANCE** from the actual value to the Regression Line.

For a given point at some X-value, the `residual` is:
`Residual = Y - Ŷ`
which Y is its **actual y value**, and Ŷ is the **y value on regression line**.

So when the `residual` is **positive**, the **actual point** is **ABOVE** the `regression line`,
and when the `residual` is **negative**, the **actual point** is **BELOW** the `regression line`.


The way that we calculate the `Regression Line` with `Least Square` method, is to **MINIMIZE** the **square of residuals**.


## Calculate the Regression Line
[Refer to Khan academy: Calculating the equation of a regression line](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/modal/v/calculating-the-equation-of-a-regression-line)

Formula of Regression line:
![image](https://user-images.githubusercontent.com/14041622/43883674-7cca8814-9be6-11e8-9d17-32ebbc411264.png)

The **SLOPE** `m` of Regression Line is NOT really equal to Correlation Coefficient `r`, 
but we have to multiply `r` with the **ratio** of Standard Deviation of `y` & `x`, which is `Sy/Sx`.


A "must go through point" is the **MEAN** of the dataset, which is: `(Ẋ, Ẏ)`

