# `Least Square Regression`
> Trying to fit a line as closely as possible, and as many of points as possible, is called "Linear Regression".

[Refer to Khan academy: Introduction to residuals and least-squares regression](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/modal/v/regression-residual-intro)


## `Residuals`
It is the **DISTANCE** from the actual value to the Regression Line.

For a given point at some X-value, the `residual` is:
`Residual = Y - Ŷ`
which Y is its **actual y value**, and Ŷ is the **y value on regression line**.

So when the `residual` is **positive**, the **actual point** is **ABOVE** the `regression line`,
and when the `residual` is **negative**, the **actual point** is **BELOW** the `regression line`.


The way that we calculate the `Regression Line` with `Least Square` method, is to **MINIMIZE** the **square of residuals**.


## Calculate the Regression Line


[Refer to Khan academy: Introduction to residuals and least-squares regression](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/modal/v/regression-residual-intro)

Formula of `Correlation coefficient r`:
![image](https://user-images.githubusercontent.com/14041622/43883674-7cca8814-9be6-11e8-9d17-32ebbc411264.png)

A "must go through point" is the **MEAN** of the dataset, which is: `(Ẋ, Ẏ)`

