# `Linear Regression`
> **Most** regressions' calculations are done by **computer**, but we want to do that by hand to have better understanding.

Trying to fit a line as closely as possible, and as many of points as possible, is called "Linear Regression".

[Refer to Khan academy: Introduction to residuals and least-squares regression](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/modal/v/regression-residual-intro)



## `Residuals`
It is the **DIFFERENCE** from the actual value to the Regression Line.

For a given point at some X-value, the `residual` is:
`Residual = Y - Ŷ`
which Y is its **actual y value**, and Ŷ is the **y value on regression line**.

So when the `residual` is **positive**, the **actual point** is **ABOVE** the `regression line`,
and when the `residual` is **negative**, the **actual point** is **BELOW** the `regression line`.


The way that we calculate the `Regression Line` with `Least Square` method, is to **MINIMIZE** the **square of residuals**.

## `Least Square Regression`
> `Least Square` is a the most common method to calculate the Linear Regression Line.

[Refer to Khan academy: Calculating the equation of a regression line](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/modal/v/calculating-the-equation-of-a-regression-line)

Formula of Regression line:
![image](https://user-images.githubusercontent.com/14041622/43884348-77a305d0-9be8-11e8-9fc1-5bd881686fb4.png)

1. As we said the `Correlation Coefficient r` is kind like the **`Unit Slope`** which is between `-1 to 1`, so we have to apply the `unit slope` in real case by multiply `r` with the **ratio** of Standard Deviation of `y` & `x`, which is `Sy/Sx`.
2. A "must go through point" is the **MEAN** of the dataset, which is: `(Ẋ, Ẏ)`

With two informations above, we can easily calculate out the estimated Regression Line.